{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Classifying newswires 新闻多分类\n",
    "上一节中，我们介绍了如何用密集连接的神经网络将向量输入划分为两个互斥的类别，但如果类别不止两个，要怎么做？\n",
    "\n",
    "本节你会构建一个网络，将路透社新闻划分为46 个互斥的主题。因为有多个类别，所以这是多分类（multiclass classification）问题的一个例子。因为每个数据点只能划分到一个类别，所以更具体地说，这是单标签、多分类（single-label, multiclass classification）问题的一个例子。如果每个数据点可以划分到多个类别（主题），那它就是一个多标签、多分类（multilabel,multiclass classification）问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据集\n",
    "本节使用路透社数据集，它包含许多短新闻及其对应的主题，由路透社在1986 年发布。它是一个简单的、广泛使用的文本分类数据集。它包括46 个不同的主题：某些主题的样本更多，但训练集中每个主题都有至少10 个样本。数据集本身也是Keras内置的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "# 与IMDB 数据集一样，参数num_words=10000 将数据限定为前10 000 个最常出现的单词。\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8982 个训练样本和2246 个测试样本\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 56,\n",
       " 5539,\n",
       " 925,\n",
       " 149,\n",
       " 8,\n",
       " 16,\n",
       " 23,\n",
       " 931,\n",
       " 3875,\n",
       " 25,\n",
       " 116,\n",
       " 5,\n",
       " 165,\n",
       " 15,\n",
       " 10,\n",
       " 67,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 5311,\n",
       " 400,\n",
       " 81,\n",
       " 79,\n",
       " 457,\n",
       " 145,\n",
       " 22,\n",
       " 331,\n",
       " 28,\n",
       " 3026,\n",
       " 331,\n",
       " 61,\n",
       " 3609,\n",
       " 2097,\n",
       " 5311,\n",
       " 79,\n",
       " 64,\n",
       " 85,\n",
       " 1863,\n",
       " 84,\n",
       " 22,\n",
       " 44,\n",
       " 6206,\n",
       " 2275,\n",
       " 79,\n",
       " 296,\n",
       " 1384,\n",
       " 157,\n",
       " 5539,\n",
       " 8,\n",
       " 16,\n",
       " 23,\n",
       " 3875,\n",
       " 4,\n",
       " 116,\n",
       " 6,\n",
       " 837,\n",
       " 5311,\n",
       " 6,\n",
       " 3834,\n",
       " 31,\n",
       " 248,\n",
       " 1032,\n",
       " 8757,\n",
       " 4,\n",
       " 1618,\n",
       " 5,\n",
       " 37,\n",
       " 38,\n",
       " 1639,\n",
       " 27,\n",
       " 358,\n",
       " 37,\n",
       " 38,\n",
       " 4716,\n",
       " 9,\n",
       " 6,\n",
       " 9474,\n",
       " 4,\n",
       " 316,\n",
       " 9,\n",
       " 662,\n",
       " 5,\n",
       " 4,\n",
       " 765,\n",
       " 5,\n",
       " 291,\n",
       " 58,\n",
       " 60,\n",
       " 2660,\n",
       " 1067,\n",
       " 136,\n",
       " 4,\n",
       " 384,\n",
       " 292,\n",
       " 270,\n",
       " 120,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与IMDB 评论一样，每个样本都是一个整数列表（表示单词索引）。\n",
    "train_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码数据\n",
    "可以使用与上一个例子相同的代码将数据向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签向量化有两种方法：你可以将标签列表转换为整数张量，或者使用one-hot 编码。one-hot 编码是分类数据广泛使用的一种格式，也叫分类编码（categorical encoding）。6.1 节给出了one-hot 编码的详细解释。在这个例子中，标签的one-hot 编码就是将每个标签表示为全零向量，只有标签索引对应的元素为1。其代码实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i ,label in enumerate(labels):\n",
    "        results[i,label] = 1.\n",
    "    return results\n",
    "one_hot_train_labels = to_one_hot(train_labels)  # 将训练标签向量化\n",
    "one_hot_test_labels = to_one_hot(test_labels)    # 将测试标签向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 46)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，Keras 内置方法可以实现这个操作，你在MNIST 例子中已经见过这种方法。            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络\n",
    "这个主题分类问题与前面的电影评论分类问题类似，两个例子都是试图对简短的文本片段进行分类。但这个问题有一个新的约束条件：输出类别的数量从2 个变为46 个。输出空间的维度要大得多。\n",
    "\n",
    "对于前面用过的Dense 层的堆叠，每层只能访问上一层输出的信息。如果某一层丢失了与分类问题相关的一些信息，那么这些信息无法被后面的层找回，也就是说，每一层都可能成为信息瓶颈。上一个例子使用了16 维的中间层，但对这个例子来说16 维空间可能太小了，无法学会区分46 个不同的类别。这种维度较小的层可能成为信息瓶颈，永久地丢失相关信息。\n",
    "\n",
    "出于这个原因，下面将使用维度更大的层，包含64 个单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于这个架构还应该注意另外两点。\n",
    "\n",
    " 网络的最后一层是大小为 46 的 Dense 层。这意味着，对于每个输入样本，网络都会输\n",
    "出一个46 维向量。这个向量的每个元素（即每个维度）代表不同的输出类别。\n",
    "\n",
    " 最后一层使用了 softmax 激活。你在 MNIST 例子中见过这种用法。网络将输出在 46\n",
    "个不同输出类别上的概率分布——对于每一个输入样本，网络都会输出一个46 维向量，\n",
    "其中output[i] 是样本属于第i 个类别的概率。46 个概率的总和为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这个例子，最好的损失函数是categorical_crossentropy（分类交叉熵）。它用于衡量两个概率分布之间的距离，这里两个概率分布分别是网络输出的概率分布和标签的真实分布。通过将这两个分布的距离最小化，训练网络可使输出结果尽可能接近真实标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证你的方法\n",
    "我们在训练数据中留出1000 个样本作为验证集\n",
    "\n",
    "留出验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在开始训练网络，共20 个轮次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 321us/step - loss: 2.5526 - acc: 0.5246 - val_loss: 1.7375 - val_acc: 0.6370\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 188us/step - loss: 1.4267 - acc: 0.7095 - val_loss: 1.3158 - val_acc: 0.7210\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 1.0580 - acc: 0.7806 - val_loss: 1.1526 - val_acc: 0.7470\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 0.8319 - acc: 0.8255 - val_loss: 1.0610 - val_acc: 0.7640\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.6625 - acc: 0.8624 - val_loss: 0.9790 - val_acc: 0.7840\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 185us/step - loss: 0.5289 - acc: 0.8903 - val_loss: 0.9439 - val_acc: 0.7890\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.4269 - acc: 0.9095 - val_loss: 0.9380 - val_acc: 0.7970\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 0.3424 - acc: 0.9261 - val_loss: 0.8820 - val_acc: 0.8070\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.2872 - acc: 0.9375 - val_loss: 0.9104 - val_acc: 0.8070\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.2392 - acc: 0.9451 - val_loss: 0.8955 - val_acc: 0.8140\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 0.2049 - acc: 0.9488 - val_loss: 0.9068 - val_acc: 0.8150\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 184us/step - loss: 0.1835 - acc: 0.9503 - val_loss: 0.9397 - val_acc: 0.8140\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 0.1633 - acc: 0.9560 - val_loss: 0.9709 - val_acc: 0.8120\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 198us/step - loss: 0.1489 - acc: 0.9554 - val_loss: 0.9616 - val_acc: 0.8020\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.1379 - acc: 0.9558 - val_loss: 0.9684 - val_acc: 0.8220\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 185us/step - loss: 0.1280 - acc: 0.9579 - val_loss: 0.9612 - val_acc: 0.8130\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 185us/step - loss: 0.1238 - acc: 0.9588 - val_loss: 0.9906 - val_acc: 0.8080\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 186us/step - loss: 0.1205 - acc: 0.9569 - val_loss: 1.0271 - val_acc: 0.8130\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 186us/step - loss: 0.1177 - acc: 0.9573 - val_loss: 1.0166 - val_acc: 0.8100\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 209us/step - loss: 0.1081 - acc: 0.9592 - val_loss: 1.0619 - val_acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制训练损失和验证损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2657274ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(loss)+1)\n",
    "plt.plot(epochs,loss,'bo', label='Training loss')\n",
    "plt.plot(epochs,val_loss,'b', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制训练精度和验证精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x265727d4a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络在训练9 轮后开始过拟合。我们从头开始训练一个新网络，共9 个轮次，然后在测试集上评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从头开始重新训练一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 2s 216us/step - loss: 2.5396 - acc: 0.5226 - val_loss: 1.6733 - val_acc: 0.6570\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 1.3712 - acc: 0.7117 - val_loss: 1.2746 - val_acc: 0.7200\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 1.0130 - acc: 0.7781 - val_loss: 1.1314 - val_acc: 0.7510\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.7975 - acc: 0.8254 - val_loss: 1.0534 - val_acc: 0.7580\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 177us/step - loss: 0.6390 - acc: 0.8634 - val_loss: 0.9762 - val_acc: 0.7940\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 0.5111 - acc: 0.8925 - val_loss: 0.9089 - val_acc: 0.8140\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.4107 - acc: 0.9147 - val_loss: 0.8905 - val_acc: 0.8200\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.3344 - acc: 0.9292 - val_loss: 0.8712 - val_acc: 0.8290\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 178us/step - loss: 0.2775 - acc: 0.9367 - val_loss: 0.9364 - val_acc: 0.8010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26572d16400>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(partial_x_train,partial_y_train,epochs=9,batch_size=512,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 1s 281us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0248980018992777, 0.7764915405695499]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test,one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在新数据上生成预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0033132e-05, 8.5511281e-05, 2.6822054e-05, ..., 3.2219876e-05,\n",
       "        6.5403738e-06, 9.7547336e-06],\n",
       "       [8.6128834e-04, 1.9036099e-02, 3.8210184e-03, ..., 5.9888174e-04,\n",
       "        1.2981832e-05, 2.9760136e-04],\n",
       "       [1.7774410e-03, 7.6527029e-01, 2.9769260e-03, ..., 1.5738426e-04,\n",
       "        1.9118023e-03, 2.1999619e-04],\n",
       "       ...,\n",
       "       [1.1235362e-05, 2.1618542e-04, 4.0543520e-05, ..., 9.1064512e-06,\n",
       "        1.1658003e-04, 1.1625566e-05],\n",
       "       [1.3548984e-03, 1.2299884e-01, 3.5364276e-03, ..., 6.1818131e-04,\n",
       "        7.5275544e-04, 3.8147951e-04],\n",
       "       [8.9320680e-04, 5.3720850e-01, 2.0211563e-03, ..., 5.3426706e-05,\n",
       "        4.8396137e-04, 2.0715705e-04]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions 中的每个元素都是长度为46 的向量。\n",
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个向量的所有元素总和为1。\n",
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最大的元素就是预测类别，即概率最大的类别。\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理标签和损失的另一种方法\n",
    "前面提到了另一种编码标签的方法，就是将其转换为整数张量，如下所示。\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "对于这种编码方法，唯一需要改变的是损失函数的选择。对于代码清单3-21 使用的损失函数categorical_crossentropy，标签应该遵循分类编码。对于整数标签，你应该使用sparse_categorical_crossentropy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个新的损失函数在数学上与categorical_crossentropy 完全相同，二者只是接口不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中间层维度足够大的重要性\n",
    "\n",
    "前面提到，最终输出是46 维的，因此中间层的隐藏单元个数不应该比46 小太多。现在来看一下，如果中间层的维度远远小于46（比如4 维），造成了信息瓶颈，那么会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 具有信息瓶颈的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 271us/step - loss: 3.5511 - acc: 0.1936 - val_loss: 3.3091 - val_acc: 0.2380\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 184us/step - loss: 3.1229 - acc: 0.2601 - val_loss: 2.9541 - val_acc: 0.2570\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 2.7377 - acc: 0.2790 - val_loss: 2.6204 - val_acc: 0.2830\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 2.3876 - acc: 0.3076 - val_loss: 2.3089 - val_acc: 0.3960\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 2.0411 - acc: 0.5801 - val_loss: 1.9826 - val_acc: 0.6090\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 1.7230 - acc: 0.6427 - val_loss: 1.7554 - val_acc: 0.6170\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 1.5039 - acc: 0.6550 - val_loss: 1.6185 - val_acc: 0.6300\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 1.3657 - acc: 0.6640 - val_loss: 1.5431 - val_acc: 0.6360\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 184us/step - loss: 1.2660 - acc: 0.6730 - val_loss: 1.5089 - val_acc: 0.6360\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 184us/step - loss: 1.1889 - acc: 0.6961 - val_loss: 1.4778 - val_acc: 0.6590\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 1.1233 - acc: 0.7270 - val_loss: 1.4507 - val_acc: 0.6710\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 1.0658 - acc: 0.7368 - val_loss: 1.4297 - val_acc: 0.6770\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 1.0167 - acc: 0.7420 - val_loss: 1.4192 - val_acc: 0.6820\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 219us/step - loss: 0.9731 - acc: 0.7458 - val_loss: 1.4298 - val_acc: 0.6820\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.9320 - acc: 0.7477 - val_loss: 1.4172 - val_acc: 0.6880\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 0.8960 - acc: 0.7533 - val_loss: 1.4130 - val_acc: 0.6840\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.8617 - acc: 0.7572 - val_loss: 1.4238 - val_acc: 0.6850\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 0.8283 - acc: 0.7586 - val_loss: 1.4154 - val_acc: 0.6870\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.7992 - acc: 0.7626 - val_loss: 1.4337 - val_acc: 0.6920\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.7677 - acc: 0.7719 - val_loss: 1.4501 - val_acc: 0.6850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26573a17cf8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(4,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在网络的验证精度最大约为68.5%，比前面下降了8%。导致这一下降的主要原因在于，你试图将大量信息（这些信息足够恢复46 个类别的分割超平面）压缩到维度很小的中间空间。网络能够将大部分必要信息塞入这个四维表示中，但并不是全部信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进一步的实验\n",
    "尝试使用更多或更少的隐藏单元，比如 32 个、128 个等。\n",
    "\n",
    "前面使用了两个隐藏层，现在尝试使用一个或三个隐藏层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 220us/step - loss: 3.0507 - acc: 0.5104 - val_loss: 2.2591 - val_acc: 0.6520\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 1.8738 - acc: 0.6884 - val_loss: 1.6095 - val_acc: 0.6780\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 188us/step - loss: 1.3692 - acc: 0.7310 - val_loss: 1.3480 - val_acc: 0.7200\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 188us/step - loss: 1.0958 - acc: 0.7707 - val_loss: 1.2083 - val_acc: 0.7420\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.9167 - acc: 0.8088 - val_loss: 1.1178 - val_acc: 0.7630\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 0.7712 - acc: 0.8404 - val_loss: 1.0529 - val_acc: 0.7880\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 0.6530 - acc: 0.8648 - val_loss: 0.9998 - val_acc: 0.8000\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.5542 - acc: 0.8836 - val_loss: 0.9733 - val_acc: 0.7950\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.4745 - acc: 0.9005 - val_loss: 0.9579 - val_acc: 0.7980\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 0.4085 - acc: 0.9163 - val_loss: 0.9272 - val_acc: 0.8180\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 0.3527 - acc: 0.9247 - val_loss: 0.9339 - val_acc: 0.8050\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 0.3080 - acc: 0.9330 - val_loss: 0.9148 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 188us/step - loss: 0.2709 - acc: 0.9382 - val_loss: 0.9467 - val_acc: 0.8140\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.2362 - acc: 0.9450 - val_loss: 0.9346 - val_acc: 0.8110: 0.2368 - acc: 0.944\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.2158 - acc: 0.9468 - val_loss: 0.9636 - val_acc: 0.8170\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.1957 - acc: 0.9478 - val_loss: 0.9456 - val_acc: 0.8160\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 188us/step - loss: 0.1751 - acc: 0.9529 - val_loss: 1.0124 - val_acc: 0.8030\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.1643 - acc: 0.9519 - val_loss: 0.9904 - val_acc: 0.8070 0.1597 - acc: 0.953\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 0.1508 - acc: 0.9528 - val_loss: 1.0041 - val_acc: 0.8090\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.1421 - acc: 0.9549 - val_loss: 1.0111 - val_acc: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x265737f85f8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 更少的隐藏单元\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度提高到80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 230us/step - loss: 2.5566 - acc: 0.4872 - val_loss: 1.6922 - val_acc: 0.6300\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 1.3902 - acc: 0.7012 - val_loss: 1.2726 - val_acc: 0.7090\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 194us/step - loss: 1.0288 - acc: 0.7781 - val_loss: 1.1113 - val_acc: 0.7670\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 211us/step - loss: 0.7989 - acc: 0.8302 - val_loss: 1.0009 - val_acc: 0.7970\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.6272 - acc: 0.8673 - val_loss: 0.9718 - val_acc: 0.7830\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 194us/step - loss: 0.4962 - acc: 0.8934 - val_loss: 0.8835 - val_acc: 0.8130\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 194us/step - loss: 0.3890 - acc: 0.9193 - val_loss: 0.8995 - val_acc: 0.8030\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 0.3135 - acc: 0.9320 - val_loss: 0.8550 - val_acc: 0.8180\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 199us/step - loss: 0.2561 - acc: 0.9416 - val_loss: 0.8621 - val_acc: 0.8210\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.2182 - acc: 0.9466 - val_loss: 0.8889 - val_acc: 0.8150\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 0.1875 - acc: 0.9515 - val_loss: 0.9303 - val_acc: 0.8100\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.1692 - acc: 0.9545 - val_loss: 0.9067 - val_acc: 0.8170\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 0.1535 - acc: 0.9524 - val_loss: 0.9543 - val_acc: 0.8100\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.1416 - acc: 0.9557 - val_loss: 0.9557 - val_acc: 0.8140\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 209us/step - loss: 0.1305 - acc: 0.9569 - val_loss: 0.9830 - val_acc: 0.8120\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 188us/step - loss: 0.1231 - acc: 0.9570 - val_loss: 0.9898 - val_acc: 0.8070\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 195us/step - loss: 0.1191 - acc: 0.9582 - val_loss: 1.0014 - val_acc: 0.8120\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 216us/step - loss: 0.1180 - acc: 0.9560 - val_loss: 1.0061 - val_acc: 0.8090\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 248us/step - loss: 0.1101 - acc: 0.9610 - val_loss: 1.0343 - val_acc: 0.8010\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 246us/step - loss: 0.1078 - acc: 0.9569 - val_loss: 1.0532 - val_acc: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26574024e10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 更多的隐藏单元\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(128,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型训练到第9轮的时候，很明显的出现了过拟合\n",
    "\n",
    "下面尝试更多的隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 234us/step - loss: 3.0277 - acc: 0.4308 - val_loss: 2.0300 - val_acc: 0.6030\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 188us/step - loss: 1.6507 - acc: 0.6724 - val_loss: 1.4828 - val_acc: 0.6870\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 1.2066 - acc: 0.7373 - val_loss: 1.2839 - val_acc: 0.7160\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 0.9648 - acc: 0.7819 - val_loss: 1.1448 - val_acc: 0.7470\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.7819 - acc: 0.8156 - val_loss: 1.0629 - val_acc: 0.7840\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 0.6399 - acc: 0.8518 - val_loss: 1.0706 - val_acc: 0.7660\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 0.5194 - acc: 0.8852 - val_loss: 1.0685 - val_acc: 0.7670\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 194us/step - loss: 0.4329 - acc: 0.9032 - val_loss: 1.0355 - val_acc: 0.7900\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 199us/step - loss: 0.3550 - acc: 0.9225 - val_loss: 0.9933 - val_acc: 0.8100\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 0.2998 - acc: 0.9346 - val_loss: 1.0150 - val_acc: 0.8020\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 203us/step - loss: 0.2579 - acc: 0.9425 - val_loss: 1.0153 - val_acc: 0.8010\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 194us/step - loss: 0.2202 - acc: 0.9474 - val_loss: 1.1349 - val_acc: 0.7690\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 194us/step - loss: 0.2039 - acc: 0.9485 - val_loss: 1.0786 - val_acc: 0.7970\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 0.1731 - acc: 0.9525 - val_loss: 1.1066 - val_acc: 0.7910\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 197us/step - loss: 0.1656 - acc: 0.9545 - val_loss: 1.1439 - val_acc: 0.7830\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.1545 - acc: 0.9560 - val_loss: 1.1030 - val_acc: 0.8060\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.1403 - acc: 0.9549 - val_loss: 1.1222 - val_acc: 0.7980\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 202us/step - loss: 0.1404 - acc: 0.9567 - val_loss: 1.1938 - val_acc: 0.7830\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 193us/step - loss: 0.1362 - acc: 0.9570 - val_loss: 1.1537 - val_acc: 0.7990\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - ETA: 0s - loss: 0.1267 - acc: 0.956 - 2s 202us/step - loss: 0.1284 - acc: 0.9558 - val_loss: 1.2195 - val_acc: 0.7990\n"
     ]
    }
   ],
   "source": [
    "# 更多的隐藏单元\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多的隐藏层并不能够提高模型精度，同样是训练到第9轮的时候出现了过拟合，后续可以把训练轮数控制在10轮以内，修改其他参数或使用其他算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "下面是你应该从这个例子中学到的要点。\n",
    "\n",
    " 如果要对 N个类别的数据点进行分类，网络的最后一层应该是大小为N的Dense层。\n",
    "\n",
    " 对于单标签、多分类问题，网络的最后一层应该使用softmax 激活，这样可以输出在N个输出类别上的概率分布。\n",
    "\n",
    " 这种问题的损失函数几乎总是应该使用分类交叉熵。它将网络输出的概率分布与目标的真实分布之间的距离最小化。\n",
    "\n",
    " 处理多分类问题的标签有两种方法。\n",
    "\n",
    "     通过分类编码（也叫 one-hot 编码）对标签进行编码，然后使用 categorical_crossentropy 作为损失函数。\n",
    "     将标签编码为整数，然后使用 sparse_categorical_crossentropy损失函数。\n",
    " 如果你需要将数据划分到许多类别中，应该避免使用太小的中间层，以免在网络中造成信息瓶颈。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
